\documentclass[11pt]{article}
%\usepackage[margin=1.25in]{geometry}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{enumerate,fullpage,amsmath,amssymb}
\newcommand{\squishlist}{
\begin{list}{$\bullet$}
{ \setlength{\itemsep}{0pt} \setlength{\parsep}{3pt}
\setlength{\topsep}{3pt} \setlength{\partopsep}{0pt}
\setlength{\leftmargin}{1.5em} \setlength{\labelwidth}{1em}
\setlength{\labelsep}{0.5em} } }
\newcommand{\squishend}{
\end{list} }
\pagestyle{empty}


% Used for JavaScript Code
% \documentclass{article}
\usepackage{listings}
\usepackage{color}
\definecolor{lightgray}{rgb}{.9,.9,.9}
\definecolor{darkgray}{rgb}{.4,.4,.4}
\definecolor{purple}{rgb}{0.65, 0.12, 0.82}

%for comments and what
\usepackage{verbatim}

\lstdefinelanguage{JavaScript}{
  keywords={typeof, new, true, false, catch, function, return, null, catch, switch, var, if, in, while, do, else, case, break},
  keywordstyle=\color{blue}\bfseries,
  ndkeywords={class, export, boolean, throw, implements, import, this},
  ndkeywordstyle=\color{darkgray}\bfseries,
  identifierstyle=\color{black},
  sensitive=false,
  comment=[l]{//},
  morecomment=[s]{/*}{*/},
  commentstyle=\color{purple}\ttfamily,
  stringstyle=\color{red}\ttfamily,
  morestring=[b]',
  morestring=[b]"
}

\lstset{
   language=JavaScript,
   backgroundcolor=\color{lightgray},
   extendedchars=true,
   basicstyle=\footnotesize\ttfamily,
   showstringspaces=false,
   showspaces=false,
   numbers=left,
   numberstyle=\footnotesize,
   numbersep=9pt,
   tabsize=2,
   breaklines=true,
   showtabs=false,
   captionpos=b
}


\newcommand{\points}[1]{\textbf{[#1 Points]}}
\newcommand{\extracredit}{\mbox{\textbf{[Extra Credit]}}}

\begin{document}

\title{CS 186 Assignment 7:\\
Programming The Crowd}
\author{Tiffany (Haotian) Wu and Luis Antonio Perez, Harvard University\\
Out Monday April 6, 2015\\
Due {\bf 5pm} sharp: {\bf Tuesday April 14, 2015}\\
(Extension School: Wednesday April 15)
}
\date{}

\maketitle

\smallskip

\begin{enumerate}
\item \points{10} {\bf Introduction}

Your goal in this part of the exercise is to learn about AMT and the TurkIt framework
and get acquainted with the basic terminology (worker, requester, HIT, etc)
\begin{enumerate}

\item[(a)] \points{4} 
The name "Mechanical Turk" is a reference to a chess-playing automata of the 18th century that toured Europe beating people, and was later revealed to be a chess master hidden in the machine. Similarly, human computing is actually utilized in these tasks that are (in this day and age) hard for computers to be good at.

The slogan of ``Artificial Artificial Intelligence'' is intended to be a play on the development of artificial intelligence \footnote{\href{http://en.wikipedia.org/wiki/Amazon_Mechanical_Turk}{Wikipedia on Amazon Mechanical Turk}}. Amazon turk accomplishes tasks typically associated with artificial intelligence through the use of actual humans, thereby making it non-artifical, or, to use the slogal, ``artificial artificial intelligence''. The slogan is clever marketing as it demonstrates an embracement of the inability to achieve ``true'' artificial intelligence. For more, see \href{https://www.mturk.com/mturk/help?helpPage=main}{FAQs}.

%
\item[(b)] \points{3} 
The last task involved giving a description of an image. The task was accomplished by navigating to the page for the generated HIT, which displayed an image of leaves and sticks, and then inputting the description into a single text box, followed by submission.

After 30 seconds, TurKit re-ran the program and captured the results from the first accomplished hit, generating a new hit which asked users to verify the input from the first. 

%
\item[(c)] \points{3} 

Link: \href{http://www.theverge.com/2014/12/4/7331777/amazon-mechanical-turk-workforce-digital-labor}{A Digital Laborforce}

Here, we have an article about people who chose MTurk as careers who wish to be recognized as real human beings, and not just an algorithm for completing simple tasks. Amazon often markets MTurk as a resource for getting small tasks completed, but sometimes paints workers as unskilled laborers who don't deserve appreciation for their contributions. Workers often feel unappreciated and unsupported as Amazon does not provide support in the case of scams among other things. 

I think it's very interesting that MTurk has evolved to have these sorts of problems, but I wonder that even if Amazon changes their marketing strategy, or people begin to be more cognizant of the workers their HITs are completed by, that anything will change.
\end{enumerate}


\item \points{6} {\bf Javascript primer }
\begin{enumerate}
\item \points{2} 
We can rewrite line 30 as follows, which, with only three characters, defines the same functionality:
\begin{lstlisting}
obj.add = add 
\end{lstlisting}

%Open the \texttt{primer.html} attached with this code package with a text editor.
%You will see the \texttt{<script..></script>} where Javascript code is located. Read and 
%understand what the code is doing and then open the file with a browser. 
%As an exercise rewrite Line 30 (\texttt{obj.add = X}) by replacing $X$ with only 4 characters such that 
%the code maintains the same functionality. 
\item \points{4} 
The object of type algebra can be defined as follows:
\begin{lstlisting}
var Algebra = {
  operation : "multiplication",
  mul : function (x,y){return x*y},
  genprod : function prod(n,f){
    if(n==0) {
      return 1;
    }
    else {
      var r = prod(n-1,f);
      return r * f(n);
    }
  }
 };
\end{lstlisting}
From the above, we can see that we have a (i) a property \texttt{operation} equal to the string ``multiplication", (ii)  a property \texttt{mul} that is a function
to compute the product of its two arguments, and (iii) a property \texttt{genprod}. 

As for (iv), \texttt{Algebra.genprod(n, function(x) \{ return x \})} calculates the $n!$.

Note: Some might be curious as to why we didn't simply use \texttt{ this } to reference the defined functions recursively. The explanation for the curiosity can be found \href{http://stackoverflow.com/questions/9006587/javascript-object-literal-method-recursive-call}{here}, where our method is explained as superior.

\end{enumerate}
%
\item \points{8} {\bf Login/Setup}

\begin{enumerate}
\item \points{1} Tifanny created the requester account, I created the worker account. Evidence for this can be found through the screenshots for other problem answers. However, see Figure \ref{fig:worker_account} for direct proof.

\begin{figure}[!h]
\centering
\includegraphics[scale=0.5]{worker_account}
\caption{Screen shot of worker account dashboard for Amazon Turk}
\label{fig:worker_account}
\end{figure}

\item \points{2} 
The experience was interesting, and just as I remember it previously. The user interface is rather poor.

At first, I had some difficulty accepting hits due to the interface, but after a while, I figured out how. My first hit consisted of the simple problem of submitting where I lived. I was paid $\$0.02$ for this hit, which is interesting, considering that most of the time, I give out this information willingly. The hit can be see in Figure \ref{fig:state_survey}.

\begin{figure}[!h]
\centering
\includegraphics[scale=0.5]{state_survey}
\caption{My first HIT done for the benefit of cs186!}
\label{fig:state_survey}
\end{figure}

Another intersting event that occured was that for some hits, you had to submit a CAPTCHA (weird? is it possible that some people have figured out how to automate some of the tasks on mTurk)?. As can be seen from Figure \ref{fig:accept_hit}, you're sometimes asked to verify that you're human before you can accept a hit.

\begin{figure}[!h]
\centering
\includegraphics[scale=0.5]{accept_hit}
\caption{Asked to verify that I am human! Isn't that crazy!}
\label{fig:accept_hit}
\end{figure}

Another curiosity about the entire experience is that there appears to be (1) a severe lack of well-paying HITs as well as (2) a huge number of hits that require prior qualifications. Either way, if you sort the hits, by reward, the first thing that pops out is that all of the hits high paying hits are by the same organization (something called SpeechPad). This information is summarized in Figure \ref{fig:qualifications} and Figure \ref{fig:speech_pad}.

\begin{figure}[!h]
\centering
\includegraphics[scale=0.5]{high_reward}
\includegraphics[scale=0.5]{parent_hit}
\caption{Most high paying hits in mTurk require previous qualifications. Some of them even require you to fit certain social criteria (be a parent).}
\label{fig:qualifications}
\end{figure}


\begin{figure}[!h]
\centering
\includegraphics[scale=0.5]{speech_pad}
\caption{The top paying hits are all by the same company/organization - SpeechPad.}
\label{fig:speech_pad}
\end{figure}

In the end, I was able to find some relatively high paying taskts which involved clicking on advertisements on other sites. I don't immediately see the purporse of these tasks, but they paid on average $\$1.00$ and took about $5$ minutes to complete, which seems to be a good return compared to other HITs on the site. As can be seen in Figure \ref{fig:results} , I didn't get that much work done or that much money. It seems like 20 minutes is just not enough. A lot of it was spent just looking for hits rather than making money. 

\begin{figure}[!h]
\centering
\includegraphics[scale=0.5]{hit_total}
\includegraphics[scale=0.5]{three_hits}
\caption{Total payment (includes some from previous uses of mTurk) as well as hits completed within the 20 minute period. It seems like the hits take longer to complete than expected. A lot of time is spent looking for more HITs.}
\label{fig:results}
\end{figure}

\item \points{2} See Figure \ref{fig:create_access_key} and \ref{fig:downloaded_TurkIT}

\begin{figure}[!h]
\centering
\includegraphics[scale=.3]{create_access_key.png}
\caption{Creating an Access Key Pair}
\label{fig:create_access_key}
\end{figure}

\begin{figure}[!h]
\centering
\includegraphics[scale=.25]{downloaded.png}
\caption{TurkIt Framework is a go.}
\label{fig:downloaded_TurkIT}
\end{figure}

\begin{figure}[!h]
\centering
\includegraphics[scale=.25]{docs.png}
\caption{Reviewing the docs for \texttt{mturk} object}
\label{fig:review_docs}
\end{figure}

% 
\item \points{1} See Figure \ref{fig:review_docs} for browsing  \url{http://groups.csail.mit.edu/uid/turkit/jsdocs/symbols/MTurk.html}
%
\item \points{2} 
See Figure \ref{fig:load_money} for loading \$7.

See Figure \ref{fig:mode_real} for changing the parameters in TurkIt to include our new AWS Key as well as setting it to run in ``real'' mode. 

Below, you will be able to find the solution to the wamr-up problem which allows us to view our balance. The balance on our account on first run was $\$7.00$:
\begin{lstlisting}
print("Hello MTurk!")
print("Your balance is: " + mturk.getAccountBalance()  )
print("No. of hits: "+ mturk.getHITs().length )

// Explore the HIT object. 
// Get the first HIT and then iterate over its properties.
if(mturk.getHITs().length > 0) {
  var hitObj = mturk.getHIT(0, true) 
  for(attr in hitObj) 
    print("Attr ="+attr+ " val="+hitObj[attr])
}
\end{lstlisting}
\begin{figure}[!h]
\centering
\includegraphics[scale=.25]{visa.png}
\caption{Purchasing Prepaid HITs}
\label{fig:load_money}
\end{figure}


\begin{figure}[!h]
\centering
\includegraphics[scale=.25]{real.png}
\caption{Changing maxMoney and mode of TurkIt.}
\label{fig:mode_real}
\end{figure}
%This will print the available amount of money in your account. 
\end{enumerate}


\item \points{10} {\bf The ``Sandbox"}
\begin{enumerate}
\item \points{4}
In Figure \ref{fig:sandbox_hit} you can find our HIT in the sandbox:
%
\begin{figure}[h]
\begin{center}
\includegraphics[scale=0.25]{sandbox_hit.png}
\end{center}
\caption{Our task in the sandbox.}
\label{fig:sandbox_hit}
\end{figure}
%
And here is the parameter code that we added. Note the additional \texttt{makeSecure} function required due to mTurk's new security features:
\begin{lstlisting}
function makeSecure(url) {
  return url.substring(0, 4) + "s" + url.substring(4);
}

var hitParams = {
  title : "Pick a Number",
  desc : "Pick a random number between 1 and 10.",
  url : makeSecure(page),
  height : 800,
  reward : 0.02
}
\end{lstlisting}

\item \points{6} 
\begin{figure}[!h]
\begin{center}
\includegraphics[scale=0.3]{random_number}
\includegraphics[scale=0.3]{manage_random_number}
\end{center}
\caption{Posted tasks live and in the ``Manage" console}
\label{fig:random_number_hit}
\end{figure}

The screenshot of the hit can be seen in Figure \ref{fig:random_number_hit}. The answers submitted were: $\{8,5,44,7,3,..5,..6,55,5\}$, in that order. Of course, the 44, ..5,..6, and 55 answers were rejected by our code as we're looking for random numbers between 1 and 10. Interestingly enough, not removing the ``..'' from the input text box seems to have led some people to submit that with their answer (probably because they're trying to do it as soon as possible). The numbers actually appear to be relatively random, but this is a very small sample size. If we're to subcribe to behavioral economics, I'd expect that with a larger sample size, we'd begin to see inputs from the set $\{0,5,10 \}$  more often than expected due to the bias people tend to have towards those numbers. Additionally, while we asked people to pick a random number between 1 and 10, no on attempted to input a decimal. All the numbers were whole integers, though this was never explicitly required. A snippet of the code is shown below:

\begin{lstlisting}
var hitIDs = new Array(5);
for(var i = 0; i < hitIDs.length; i++){
  // create the hit
  hitIDs[i] = mturk.createHIT(hitParams)
  print("-----------------")
  print("Hit created  : "+ hitIDs[i])
  print("-----------------")
  var hit = mturk.waitForHIT(hitIDs[i])

  // verify the hit is random between 0 and 10
  var input = hit.assignments[0].answer.newText
  print("Input "+input)
  var iinput = parseInt(input)
  if (0 <= iinput && iinput <= 10){
    mturk.approveAssignment(hit.assignments[0])
  }
  else {
    mturk.rejectAssignment(hit.assignments[0])
    i--
  }
}
\end{lstlisting}

However, it still took  a long time to run and wait for all five responses (over a day). It seems that no one wanted to make quick few cents. The price definitely needed to be increased, but given the low priority of this assignment, we decided to leave the payment at $\$0.02$ and just wait it out. It's interesting to note that the assignments seem to take much longer if not immediately accepted.
\end{enumerate}

\item \points{16} {\bf Nuclei Counting}

\begin{enumerate}
\item[(a)] 
It is \texttt{params}, an object which JavaScript terms an associative array. In other languages, it's typically known as a dictionary or mapping. The parameters are:
\begin{itemize}
\item \texttt{title} - the text that is displayed in the list of HITs
\item \texttt{desc} - also shown, but not as the bolded text above
\item \texttt{question (url, height)} - a string of XML that is what will be displayed in the iframe. url can be a pointer to a page, and then that is loaded into the iframe. Heigh is self explanatory. 
\end{itemize}
%
\item[(b)] \points{3}
The code modifications we made to create the nuclei hit are shown below:
\begin{lstlisting}
 return mturk.createHIT({
        title: "Count Nuclei in Picture",
        desc : "Count the total number of nuclei (dots) in a picture. Your output will be voted on for accuracy, and your payment dependent on that vote.",
        question: q,
        reward: argCost
    })
\end{lstlisting}

%
\item[(c)] \points{5}
The code modifications are below:
\begin{lstlisting}
var voteHitId = mturk.createHIT({
        title: "Vote on Nuclei Counts in Picture",
        desc : "Vote whether or not another worker's count is accurate. Count is of nuclei in an image.",
        question: q,
        reward: argVoteCost,
        maxAssignments: 3
    });
\end{lstlisting}


\item[(d)] \points{3} We set \texttt{maxAssignments} to 3 because 2/3 votes is dependable enough. As for the payments, we can see their final value in the code below:
\begin{lstlisting}
var nucleiHITcost = 0.50
...
var votingCost = 0.10
if (vote(ansCount, votingCost)) {
    mturk.approveAssignment(hit.assignments[0])
    print("\nConsensus reached!\n")
} else {
    mturk.rejectAssignment(hit.assignments[0])
    print("\nNumber "+ansCount+" was not voted as correct\n")
}    
\end{lstlisting}
We set \texttt{argCost=0.20} initially because we figured that if inaccurate, we wouldn't have to pay, so we would only need to pay one worker for a reasonably accurate estimate. Thus, we could afford to have a higher payout for this. However, the low payout turned out to be insufficient and was later changed to \texttt{argCost=0.50}. We also set \texttt{votingCost = 0.10} because the workers will only have to verify the counts. A payment of \$0.10 seems reasonable for such a simple task.

\item[(e)] \points{4} 
We ran this twice. 

The first time, count price was \$ 0.20, and the vote reward was \$ .03. The count obtained was 150, and it took around 50 seconds for the worker to count after 30 minutes of the HIT going live. 

The votes trickled in around the next few hours, mainly because we assigned such a low reward, but the vote ultimately rejected the estimate of 150 (probably because voters assumed that the count was inaccurate.)

The second time, the count price was raised to \$0.50 to encourage accurate counts, and a response was received within 10 minutes. The votes still trickled in and continue to do so at this moment. At the end of our experimenting, our account balance was $\$5.745$.

\begin{figure}[!h]
\begin{center}
\includegraphics[scale=0.3]{nuclei_count}
\includegraphics[scale=0.3]{nuclei_vote}
\end{center}
\caption{Pictures of our Nuclei HITs - both count and vote.}
\end{figure}

\begin{figure}[!h]
\begin{center}
\includegraphics[scale=0.3]{turkit_proof}
\end{center}
\caption{The HITs in TurkIt.}
\end{figure}

\end{enumerate}

\item \points{20} {\bf Image Sorting}
For the image sorting, task the pseudo code is likely to look something like this:
\begin{lstlisting}
// modified implementatation of merge sort
function compare(x,y){
    var hit = False
    while(!hit){
        // create new hit
        // create a voting hit
        // check results
        if (voted positively){
            hit = true
        }
    }
    
    // return x if x <= y otheriwise y
}


function merge(xs,ys){
    // given two sorted arrays of images, merges them 1 by 1 using turk for comparison
    
    // compare sorted arrays, one by one, by creating a new hit per comparison
    iterate over x <- xs and y <- ys {
        // compare x with y and place into new merged array
    }
}

function mergesort(x){
    // split the list in half (unless size is 1, in which case exit)
    
    // mergesort left
    // mergesort right
    
    // return merge (left,right)
}
\end{lstlisting}
We think that merge sort is a good idea in this task simply because it requires the least number of comparsions. With this in mind, we can probably pay each worker slightly more, hopefully encouraging correctness of the sorting. Additionally, it seems like there already exists a sort function within turk, which would come in hand if it does what we thinkg it does. 

Another idea that we've had for this is to provide more context for the image sorting -- in this sense, we provide more images to be sorted at once. Behaviorally, this should help the worker more accurately sequence the images. In this scenario, we can probably break the images into all possible subsets of 4, and then have have a worker arrange each, with additional input votes from other workers. Given all of these arrangements, we can then take the best one (the one most people agreed on) for each image comparion, and sort that way. This method would not only decrease the total number of hits needed (since we're asking people to do more than one comparison per hit), but it would aso increase our accuracy because we'd achieve multiple verification methods. 

\begin{comment}
* The Below Can Be Ignored *

you're the worst. :) THE WORST.

HI :)

You love me. 

Let's be clear.

false.

Patently false.

...watch us forget to delete this and it'll end up in the final writeup.

That would be hilarious :)

I would not be against that. 

Let's do it.

Hi Professor Parkes!

Also, we should work on our project proposal.

Are you going to bed soon? Always time tomorrow. please? After 2:30, I am free.

Of course :) 

OK i set a bad example. I changed my mind. no more smiley faces.

:(

emoticons are for wimps.

You wana go? Huh. You wanna go, punk?

I'll fight you, no problem.

I've never hit a girl before. 

FISTICUFFS. 

:))))))
:)
 ew
 sucks
 :Dy
 our:De
 rwerwkelitsknfksnknosnkdvnslknvijns io ihskvnsiv
 :)
 ...
 d
 isgusting.
 Also, sorry for wokrong on this, but there was a piazza post. We needed pseudocode for 6.
 ...ew.
 thanks, brah.
 np. 
 
 Imma leave this here.NO
 \end{comment}

\end{enumerate}
\end{document}


